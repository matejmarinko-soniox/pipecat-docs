---
title: "Smart Turn Detection"
description: "Advanced conversational turn detection powered by the smart-turn model"
---

## Overview

Smart Turn Detection is an advanced feature in Pipecat that determines when a user has finished speaking and the bot should respond. Unlike basic Voice Activity Detection (VAD) which only detects speech vs. non-speech, Smart Turn Detection uses a machine learning model to recognize natural conversational cues like intonation patterns and linguistic signals.

<Card
  title="Smart Turn Model"
  icon="github"
  href="https://github.com/pipecat-ai/smart-turn"
>
  Open source model for advanced conversational turn detection. Contribute to
  model training and development.
</Card>

Pipecat provides two implementations of Smart Turn Detection:

1. **SmartTurnAnalyzer** - Uses a remote service for inference
2. **LocalCoreMLSmartTurnAnalyzer** - Runs inference locally on Apple Silicon using CoreML

Both implementations share the same underlying API and parameters, making it easy to switch between them based on your deployment requirements.

## Installation

The Smart Turn Detection feature requires additional dependencies depending on which implementation you choose.

For remote inference:

```bash
pip install "pipecat-ai[remote-smart-turn]"
```

For local inference (CoreML-based):

```bash
pip install "pipecat-ai[local-smart-turn]"
```

## Integration with Transport

Smart Turn Detection is integrated into your application by setting either `SmartTurnAnalyzer` or `LocalCoreMLSmartTurnAnalyzer` as the `end_of_turn_analyzer` parameter in your transport configuration:

```python
from pipecat.transports.base_transport import TransportParams

transport = SmallWebRTCTransport(
    webrtc_connection=webrtc_connection,
    params=TransportParams(
        # Other transport parameters...
        end_of_turn_analyzer=SmartTurnAnalyzer(url=remote_smart_turn_url),
    ),
)
```

<Tip>
  Smart Turn Detection requires VAD to be enabled and works best when the VAD analyzer is set to a short `stop_secs` value. We recommend 0.2 seconds.

```python
vad_enabled=True,
vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2))
```

</Tip>

## Configuration

Both implementations use the same `SmartTurnParams` class to configure behavior:

<ParamField path="stop_secs" type="float" default="3.0">
  Duration of silence in seconds required before triggering a silence-based end
  of turn
</ParamField>

<ParamField path="pre_speech_ms" type="float" default="0.0">
  Amount of audio (in milliseconds) to include before speech is detected
</ParamField>

<ParamField path="max_duration_secs" type="float" default="8.0">
  Maximum allowed segment duration in seconds. For segments longer than this
  value, a rolling window is used.
</ParamField>

## Remote Smart Turn

The `SmartTurnAnalyzer` class uses a remote service for turn detection inference.

### Constructor Parameters

<ParamField path="url" type="str" required>
  The URL of the remote Smart Turn service
</ParamField>

<ParamField path="sample_rate" type="Optional[int]" default="None">
  Audio sample rate (will be set by the transport if not provided)
</ParamField>

<ParamField path="params" type="SmartTurnParams" default="SmartTurnParams()">
  Configuration parameters for turn detection
</ParamField>

### Example

```python
import os
from pipecat.audio.turn.smart_turn import SmartTurnAnalyzer
from pipecat.audio.turn.base_smart_turn import SmartTurnParams
from pipecat.transports.base_transport import TransportParams

# Get the URL for the remote Smart Turn service
remote_smart_turn_url = os.getenv("REMOTE_SMART_TURN_URL")

# Create the transport with Smart Turn detection
transport = SmallWebRTCTransport(
    webrtc_connection=webrtc_connection,
    params=TransportParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_enabled=True,
        vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
        vad_audio_passthrough=True,
        end_of_turn_analyzer=SmartTurnAnalyzer(
            url=remote_smart_turn_url,
            params=SmartTurnParams(
                stop_secs=3.0,
                pre_speech_ms=0.0,
                max_duration_secs=8.0
            )
        ),
    ),
)
```

## Local Smart Turn (CoreML)

The `LocalCoreMLSmartTurnAnalyzer` runs inference locally using CoreML, providing lower latency and no network dependencies.

### Constructor Parameters

<ParamField path="smart_turn_model_path" type="str" required>
  Path to the directory containing the Smart Turn model
</ParamField>

<ParamField path="sample_rate" type="Optional[int]" default="None">
  Audio sample rate (will be set by the transport if not provided)
</ParamField>

<ParamField path="params" type="SmartTurnParams" default="SmartTurnParams()">
  Configuration parameters for turn detection
</ParamField>

### Example

```python
import os
from pipecat.audio.turn.local_smart_turn import LocalCoreMLSmartTurnAnalyzer
from pipecat.audio.turn.base_smart_turn import SmartTurnParams
from pipecat.transports.base_transport import TransportParams

# Path to the Smart Turn model directory
smart_turn_model_path = os.getenv("LOCAL_SMART_TURN_MODEL_PATH")

# Create the transport with local Smart Turn detection
transport = SmallWebRTCTransport(
    webrtc_connection=webrtc_connection,
    params=TransportParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_enabled=True,
        vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
        vad_audio_passthrough=True,
        end_of_turn_analyzer=LocalCoreMLSmartTurnAnalyzer(
            smart_turn_model_path=smart_turn_model_path,
            params=SmartTurnParams(
                stop_secs=2.0,  # Shorter stop time when using Smart Turn
                pre_speech_ms=0.0,
                max_duration_secs=8.0
            )
        ),
    ),
)
```

## Local Model Setup

To use the `LocalCoreMLSmartTurnAnalyzer`, you need to set up the CoreML model locally. Follow these steps from the [official repository instructions](https://github.com/pipecat-ai/smart-turn/blob/main/coreml/README.md):

1. Install Git LFS (Large File Storage):

   ```bash
   # On macOS
   brew install git-lfs

   # On Ubuntu/Debian
   sudo apt-get install git-lfs

   # Initialize Git LFS
   git lfs install
   ```

2. Clone the Smart Turn model repository:

   ```bash
   git clone https://huggingface.co/pipecat-ai/smart-turn
   ```

3. Set the environment variable to the cloned repository path:

   ```bash
   # Add to your .env file or environment
   export LOCAL_SMART_TURN_MODEL_PATH=/path/to/smart-turn
   ```

4. Verify the CoreML model exists:
   ```bash
   # The CoreML model should be located at:
   ls /path/to/smart-turn/coreml/smart_turn_classifier.mlpackage
   ```

Note that the CoreML model is optimized for Apple Silicon devices. If you're using a different platform, the remote Smart Turn service might be a better option.

## How It Works

Smart Turn Detection works in three main phases:

1. **Audio Collection**: When the user starts speaking, the system begins collecting audio.

2. **Speech Monitoring**: While the user is speaking, the audio is buffered along with timestamps.

3. **Turn Analysis**: When the VAD detects silence, the system:
   - Extracts the speech segment plus any pre-speech audio
   - Processes the segment through the ML model
   - Determines if the turn is complete based on linguistic and acoustic patterns

The model identifies natural completion points in speech, improving on basic silence detection by recognizing:

- Rising or falling intonation patterns
- Grammatical completeness
- Filler words ("um", "uh") that might indicate continuation
- Other acoustic markers humans naturally use to signal turn completion

## Notes

- The model is designed for English speech; performance may vary with other languages
- You can adjust the `stop_secs` parameter based on your application's needs for responsiveness
- Smart Turn generally provides a more natural conversational experience but is computationally more intensive than simple VAD
