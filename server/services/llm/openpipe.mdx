---
title: "OpenPipe"
description: "LLM service implementation using OpenPipe for LLM request logging and fine-tuning"
---

## Overview

`OpenPipeLLMService` extends the BaseOpenAILLMService to provide integration with OpenPipe, enabling request logging, model fine-tuning, and performance monitoring. It maintains compatibility with OpenAI's API while adding OpenPipe's logging and optimization capabilities.

## Installation

To use `OpenPipeLLMService`, install the required dependencies:

```bash
pip install "pipecat-ai[openpipe]"
```

You'll need to set up the following environment variables:

- `OPENPIPE_API_KEY` - Your OpenPipe API key
- `OPENAI_API_KEY` - Your OpenAI API key

## Configuration

### Constructor Parameters

<ParamField path="model" type="str" default="gpt-4o">
  Model identifier
</ParamField>

<ParamField path="api_key" type="str | None" optional>
  OpenAI API key
</ParamField>

<ParamField path="base_url" type="str | None" optional>
  OpenAI API endpoint
</ParamField>

<ParamField path="openpipe_api_key" type="str | None" optional>
  OpenPipe API key
</ParamField>

<ParamField
  path="openpipe_base_url"
  type="str"
  default="https://app.openpipe.ai/api/v1"
>
  OpenPipe API endpoint
</ParamField>

<ParamField path="tags" type="Dict[str, str] | None" optional>
  Custom tags for request logging
</ParamField>

### Input Parameters

Inherits all input parameters from BaseOpenAILLMService:

<ParamField path="extra" type="Optional[Dict[str, Any]]">
  Additional parameters to pass to the model
</ParamField>

<ParamField path="frequency_penalty" type="Optional[float]">
  Reduces likelihood of repeating tokens based on their frequency. Range: [-2.0,
  2.0]
</ParamField>

<ParamField path="max_completion_tokens" type="Optional[int]">
  Maximum number of tokens in the completion. Must be greater than or equal to 1
</ParamField>

<ParamField path="max_tokens" type="Optional[int]">
  Maximum number of tokens to generate. Must be greater than or equal to 1
</ParamField>

<ParamField path="presence_penalty" type="Optional[float]">
  Reduces likelihood of repeating any tokens that have appeared. Range: [-2.0,
  2.0]
</ParamField>

<ParamField path="seed" type="Optional[int]">
  Random seed for deterministic generation. Must be greater than or equal to 0
</ParamField>

<ParamField path="temperature" type="Optional[float]">
  Controls randomness in the output. Range: [0.0, 2.0]
</ParamField>

<ParamField path="top_p" type="Optional[float]">
  Controls diversity via nucleus sampling. Range: [0.0, 1.0]
</ParamField>

## Usage Example

```python
from pipecat.services.openpipe import OpenPipeLLMService
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext

# Configure service with tags
service = OpenPipeLLMService(
    model="gpt-4",
    openpipe_api_key="your-openpipe-key",
    tags={
        "environment": "production",
        "feature": "customer-support"
    },
    params=OpenPipeLLMService.InputParams(
        temperature=0.7,
        max_tokens=1000
    )
)

# Create context
context = OpenAILLMContext(
    messages=[
        {"role": "system", "content": "You are a helpful assistant"},
        {"role": "user", "content": "How do I optimize my API usage?"}
    ]
)

# Use in pipeline
pipeline = Pipeline([
    context_manager,  # Manages conversation context
    service,          # Processes LLM requests with logging
    text_handler      # Handles responses
])
```

## Request Logging

OpenPipe automatically logs requests with configurable tags:

```python
# Configure detailed logging
service = OpenPipeLLMService(
    tags={
        "environment": "production",
        "feature": "chat",
        "version": "v1.0",
        "user_type": "premium"
    }
)
```

## Frame Flow

Inherits the BaseOpenAI LLM Service frame flow with added logging:

```mermaid
graph TD
    A[Input Context] --> B[OpenPipeLLMService]
    B --> C[LLMFullResponseStartFrame]
    B --> D[Request Logging]
    B --> E[TextFrame Chunks]
    B --> F[Function Calls]
    B --> G[LLMFullResponseEndFrame]
    F --> H[Function Results]
    H --> B
```

## Metrics Support

The service collects standard metrics plus OpenPipe-specific data:

- Token usage (prompt and completion)
- Processing duration
- Time to First Byte (TTFB)
- Request logs and metadata
- Performance metrics
- Cost tracking

## Common Use Cases

1. **Performance Monitoring**

   - Request latency tracking
   - Token usage monitoring
   - Cost analysis

2. **Model Optimization**

   - Data collection for fine-tuning
   - Response quality monitoring
   - Usage pattern analysis

3. **Development and Testing**
   - Request logging for debugging
   - A/B testing
   - Quality assurance

## Notes

- Maintains OpenAI API compatibility
- Automatic request logging
- Support for custom tags
- Fine-tuning data collection
- Performance monitoring
- Cost tracking capabilities
- Thread-safe processing
